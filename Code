#Happy'spart

import numpy as np
import matplotlib.pyplot  as plt

#sigmoid function
def sigmoid(Z):
    return 1/(1 + np.exp(-Z))


#osama

# Cost function (Binary Cross-Entropy Loss)
def compute_cost(X, y, weights):
    m = len(y)
    z = np.dot(X, weights)
    h = sigmoid(z)
    h = np.clip(h, 1e-10, 1 - 1e-10)  # Avoid log(0)
    cost = - (1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return cost


#rakib

#Load iris data(using provided snippet)
def load_iris_data():
    # Features: Sepal lenth, Sepal width, Petal length, Petal width
    X = np.array([
    [5.1,3.5,1.4,0.2],
    [4.9,3.0,1.4,0.2],
    [4.7,3.2,1.3,0.2],
    [4.6,3.1,1.5,0.2],
    [5.0,3.6,1.4,0.2],
    [7.0,3.2,4.7,1.4],
    [6.4,3.2,4.5,1.5],
    [6.9,3.1,4.9,1.5],
    [5.5,2.3,4.0,1.3],
    [6.5,2.8,4.6,1.5],
    ])
    #Target: 0 = Setosa, 1 = Non-Setosa
    y = np.array([0,0,0,0,0,1,1,1,1,1])
    return X,y


#shakir

# standardize features
def standardize(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X-mean) / std


# Gradient Descent
def gradient_descent(x, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
    for i in range(iterations):
        z = np.dot(x, weights)
        h = sigmoid(z)
        gradients = (1/m) * np.dot(X.T, (h - y))
        weights -= learning_rate * gradients
        cost = compute_cost(x, y, weights)
        cost_history.append(cost)
        if 1 % 100 == 0:
            print(f"Iterations {i} : Cost = {cost}")
    return weights, cost_history


#Prediction Function

def predict(X, weights, threshold=0.5):
    z = np.dot(X, weights)
    h = sigmoid(z)
    return (h>= threshold).astype(int)


# Main executetion
if __name__ == "__main__":
    # Load data
    X, y = load_iris_data()
    
    # Use only petal length and petal width for visualization (indices 2 and 3)
    X_vis = X[:, [2, 3]] # Petal length, Petal width
    X_vis = standardize(X_vis) #Standize for better convergence
    
    # Hyperparameters
    learning_rate = 0.1 
    iterations = 1000
    
    # Train the model
    weights, cost_history = gradient_descent(X_vis , y, weights, learning_rate, interations)


    #Plot cost history
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.plot(cost_history)
    plt.title("Cost History")
    plt.xlabel("Iterations")
    plt.ylabel("Cost")
    
    # Plot data and dicision boundery
    plt.subplot(1, 2, 2)
    plt.scatter(X_vis[:,1], X_vis[:,2], c=y, cmap='viridis', edgecolor='k')
    plt.title("Iris Data and Decision Boundary")
    plt.xlabel("Petal Length (standardized)")
    plt.ylabel("Petal Width (standardized)")
    
    # Decision boundary: bias + w1*x1 + w2*x2 = 0 => x2 = -(bias + w1*x1)/w2
    x1 = np.linspace(np.min(X_vis[:,1]), np.max(X_vis[:, 1]), 100)
    if weights[2] != 0: 
        x2 = -(weights[0] + weights[1] * x1) / weights[2]
        plt.plot(x1, x2, 'r--', label='Decision Boundary')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

