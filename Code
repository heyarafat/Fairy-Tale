#Happy'spart

import numpy as np
import matplotlib.pyplot  as plt

#sigmoid function
def sigmoid(Z):
return 1/(1 + np.exp(-Z))


#osama

# Cost function (Binary Cross-Entropy Loss)
def compute_cost(X, y, weights):
m = len(y)
z = np.dot(X, weights)
h = sigmoid(z)
h = np.clip(h, 1e-10, 1 - 1e-10)  # Avoid log(0)
cost = - (1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
return cost


#rakib

#Load iris data(using provided snippet)
def load_iris_data():
# Features: Sepal lenth, Sepal width, Petal length, Petal width
X = np.array([
[5.1,3.5,1.4,0.2],
[4.9,3.0,1.4,0.2],
[4.7,3.2,1.3,0.2],
[4.6,3.1,1.5,0.2],
[5.0,3.6,1.4,0.2],
[7.0,3.2,4.7,1.4],
[6.4,3.2,4.5,1.5],
[6.9,3.1,4.9,1.5],
[5.5,2.3,4.0,1.3],
[6.5,2.8,4.6,1.5],
])
#Target: 0 = Setosa, 1 = Non-Setosa
y = np.array([0,0,0,0,0,1,1,1,1,1])
return X,y


#shakir

# standardize features
def standardize(X):
    mean = np.mean(X, axis=0)
    std = np.std(X, axis=0)
    return (X-mean) / std


# Gradient Descent
def gradient_descent(x, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []
for i in range(iterations):
    z = np.dot(x, weights)
    h = sigmoid(z)
    gradients = (1/m) * np.dot(X.T, (h - y))
    weights -= learning_rate * gradients
    cost = compute_cost(x, y, weights)
    cost_history.append(cost)
    if 1 % 100 == 0:
        print(f " Iterations {i} : Cost = {cost}")
  return weights, cost_history


#
      
